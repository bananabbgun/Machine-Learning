{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU","kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30886,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# ML2025 Homework 1 - Retrieval Augmented Generation with Agents","metadata":{"_uuid":"5404a277-0793-4406-b1bd-9e278c155971","_cell_guid":"51439b8a-2d94-460a-aa37-df9472bb17a1","trusted":true,"collapsed":false,"id":"1TFwaJir_Olj","jupyter":{"outputs_hidden":false}}},{"cell_type":"markdown","source":"## Environment Setup","metadata":{"_uuid":"bee31a18-9803-4a2a-93d8-204c3dcf107d","_cell_guid":"283f6071-f23d-42aa-9e3d-02429dcae72d","trusted":true,"collapsed":false,"id":"6tQHdH2k_Olk","jupyter":{"outputs_hidden":false}}},{"cell_type":"markdown","source":"In this section, we install the necessary python packages and download model weights of the quantized version of LLaMA 3.1 8B. Also, download the dataset. Note that the model weight is around 8GB.","metadata":{"_uuid":"fb222ed0-1a0c-4a53-8658-b2917ebd7bc1","_cell_guid":"bf52766f-fd1a-4bff-b0a8-7c78fcb1027a","trusted":true,"collapsed":false,"id":"mGx000oZ_Oll","jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"!python3 -m pip install --no-cache-dir llama-cpp-python==0.3.4 --extra-index-url https://abetlen.github.io/llama-cpp-python/whl/cu122\n!python3 -m pip install googlesearch-python bs4 charset-normalizer requests-html lxml_html_clean\n\nfrom pathlib import Path\nif not Path('./Meta-Llama-3.1-8B-Instruct-Q8_0.gguf').exists():\n    !wget https://huggingface.co/bartowski/Meta-Llama-3.1-8B-Instruct-GGUF/resolve/main/Meta-Llama-3.1-8B-Instruct-Q8_0.gguf\nif not Path('./public.txt').exists():\n    !wget https://www.csie.ntu.edu.tw/~ulin/public.txt\nif not Path('./private.txt').exists():\n    !wget https://www.csie.ntu.edu.tw/~ulin/private.txt","metadata":{"_uuid":"4b70c9f5-6b47-43e2-887f-feddb686e0fc","_cell_guid":"5f604853-2b68-4736-9130-e0c72af6eef4","trusted":true,"collapsed":false,"id":"5JywoPOO_Oll","execution":{"iopub.status.busy":"2025-03-06T16:59:09.419185Z","iopub.execute_input":"2025-03-06T16:59:09.419587Z","iopub.status.idle":"2025-03-06T16:59:16.179590Z","shell.execute_reply.started":"2025-03-06T16:59:09.419554Z","shell.execute_reply":"2025-03-06T16:59:16.178688Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nif not torch.cuda.is_available():\n    raise Exception('You are not using the GPU runtime. Change it first or you will suffer from the super slow inference speed!')\nelse:\n    print('You are good to go!')","metadata":{"_uuid":"66e0932c-3eaa-4f0d-8c8e-1cd7e2938103","_cell_guid":"a6f48d73-d5de-4b64-9a88-f5bef85a1438","trusted":true,"collapsed":false,"id":"kX6SizAt_Olm","execution":{"iopub.status.busy":"2025-03-06T16:59:16.180702Z","iopub.execute_input":"2025-03-06T16:59:16.180994Z","iopub.status.idle":"2025-03-06T16:59:16.186200Z","shell.execute_reply.started":"2025-03-06T16:59:16.180970Z","shell.execute_reply":"2025-03-06T16:59:16.185442Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Prepare the LLM and LLM utility function","metadata":{"_uuid":"89037220-e548-47bc-90cf-21a404103fa7","_cell_guid":"4dd15bba-6f23-4efb-bbc6-bfc3b8496093","trusted":true,"collapsed":false,"id":"l3iyc1qC_Olm","jupyter":{"outputs_hidden":false}}},{"cell_type":"markdown","source":"By default, we will use the quantized version of LLaMA 3.1 8B. you can get full marks on this homework by using the provided LLM and LLM utility function. You can also try out different LLM models.","metadata":{"_uuid":"6eb0a82a-40cf-4cfd-9b8e-c35a8bf07cbb","_cell_guid":"af435b57-2311-42fc-819a-d0d167e7944b","trusted":true,"collapsed":false,"id":"T59vxAo2_Olm","jupyter":{"outputs_hidden":false}}},{"cell_type":"markdown","source":"In the following code block, we will load the downloaded LLM model weights onto the GPU first.\nThen, we implemented the generate_response() function so that you can get the generated response from the LLM model more easily.","metadata":{"_uuid":"16958bad-b8dd-432e-99bd-67199784d414","_cell_guid":"63e1b6cb-2c5e-4a5b-a336-0b8e6079b389","trusted":true,"collapsed":false,"id":"vtepTeT3_Olm","jupyter":{"outputs_hidden":false}}},{"cell_type":"markdown","source":"You can ignore \"llama_new_context_with_model: n_ctx_per_seq (16384) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\" warning.","metadata":{"_uuid":"9d76da98-47c6-4471-9942-b5295e34d88a","_cell_guid":"0c73c577-3ec0-40ba-a145-910e3498ff64","trusted":true,"collapsed":false,"id":"eVil2Vhe_Olm","jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"from llama_cpp import Llama\n\n# Load the model onto GPU\nllama3 = Llama(\n    \"./Meta-Llama-3.1-8B-Instruct-Q8_0.gguf\",\n    verbose=False,\n    n_gpu_layers=-1,\n    n_ctx=16384,    # This argument is how many tokens the model can take. The longer the better, but it will consume more memory. 16384 is a proper value for a GPU with 16GB VRAM.\n)\n\ndef generate_response(_model: Llama, _messages: str) -> str:\n    '''\n    This function will inference the model with given messages.\n    '''\n    _output = _model.create_chat_completion(\n        _messages,\n        stop=[\"<|eot_id|>\", \"<|end_of_text|>\"],\n        max_tokens=512,    # This argument is how many tokens the model can generate.\n        temperature=0,      # This argument is the randomness of the model. 0 means no randomness. You will get the same result with the same input every time. You can try to set it to different values.\n        repeat_penalty=2.0,\n    )[\"choices\"][0][\"message\"][\"content\"]\n    return _output","metadata":{"_uuid":"3e4eb1e8-e1b8-431c-b894-7fcf1112f649","_cell_guid":"136ff81c-6cba-4753-b414-38a36d96775e","trusted":true,"collapsed":false,"id":"ScyW45N__Olm","execution":{"iopub.status.busy":"2025-03-06T16:59:16.188044Z","iopub.execute_input":"2025-03-06T16:59:16.188263Z","iopub.status.idle":"2025-03-06T16:59:17.622620Z","shell.execute_reply.started":"2025-03-06T16:59:16.188245Z","shell.execute_reply":"2025-03-06T16:59:17.621099Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Search Tool","metadata":{"_uuid":"df920ab6-fc23-4aa9-a5af-10a3a1ee6b68","_cell_guid":"292bbfd8-1347-4fdf-925c-43781a64ef27","trusted":true,"collapsed":false,"id":"tnHLwq-4_Olm","jupyter":{"outputs_hidden":false}}},{"cell_type":"markdown","source":"The TA has implemented a search tool for you to search certain keywords using Google Search. You can use this tool to search for the relevant **web pages** for the given question. The search tool can be integrated in the following sections.","metadata":{"_uuid":"477fe250-fcef-4735-b896-7e39e122fae6","_cell_guid":"1bbe6dcf-1de9-46bc-85a0-ec3df80f28f2","trusted":true,"collapsed":false,"id":"SYM-2ZsE_Olm","jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"from typing import List\nfrom googlesearch import search as _search\nfrom bs4 import BeautifulSoup\nfrom charset_normalizer import detect\nimport asyncio\nfrom requests_html import AsyncHTMLSession\nimport urllib3\nurllib3.disable_warnings()\n\nasync def worker(s:AsyncHTMLSession, url:str):\n    try:\n        header_response = await asyncio.wait_for(s.head(url, verify=False), timeout=10)\n        if 'text/html' not in header_response.headers.get('Content-Type', ''):\n            return None\n        r = await asyncio.wait_for(s.get(url, verify=False), timeout=10)\n        return r.text\n    except:\n        return None\n\nasync def get_htmls(urls):\n    session = AsyncHTMLSession()\n    tasks = (worker(session, url) for url in urls)\n    return await asyncio.gather(*tasks)\n\nasync def search(keyword: str, n_results: int=3) -> List[str]:\n    '''\n    This function will search the keyword and return the text content in the first n_results web pages.\n    Warning: You may suffer from HTTP 429 errors if you search too many times in a period of time. This is unavoidable and you should take your own risk if you want to try search more results at once.\n    The rate limit is not explicitly announced by Google, hence there's not much we can do except for changing the IP or wait until Google unban you (we don't know how long the penalty will last either).\n    '''\n    keyword = keyword[:100]\n    # First, search the keyword and get the results. Also, get 2 times more results in case some of them are invalid.\n    results = list(_search(keyword, n_results * 2, lang=\"zh\", unique=True))\n    # Then, get the HTML from the results. Also, the helper function will filter out the non-HTML urls.\n    results = await get_htmls(results)\n    # Filter out the None values.\n    results = [x for x in results if x is not None]\n    # Parse the HTML.\n    results = [BeautifulSoup(x, 'html.parser') for x in results]\n    # Get the text from the HTML and remove the spaces. Also, filter out the non-utf-8 encoding.\n    results = [''.join(x.get_text().split()) for x in results if detect(x.encode()).get('encoding') == 'utf-8']\n    # Return the first n results.\n    return results[:n_results]","metadata":{"_uuid":"3a70204b-f63e-4e9e-9ced-f481ee510d6e","_cell_guid":"483440d0-fdb4-4623-82f8-ece138803a05","trusted":true,"collapsed":false,"id":"bEIRmZl7_Oln","execution":{"iopub.status.busy":"2025-03-06T16:59:17.623296Z","iopub.status.idle":"2025-03-06T16:59:17.623601Z","shell.execute_reply":"2025-03-06T16:59:17.623469Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Test the LLM inference pipeline","metadata":{"_uuid":"920717e0-6ca1-4bf1-9983-7ceebe73c505","_cell_guid":"c4741ab6-65e1-4b1c-b297-cb49dc4a8bef","trusted":true,"collapsed":false,"id":"rC3zQjjj_Oln","jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"# You can try out different questions here.\ntest_question='請問誰是 Taylor Swift？'\n\nmessages = [\n    {\"role\": \"system\", \"content\": \"你是 LLaMA-3.1-8B，是用來回答問題的 AI。使用中文時只會使用繁體中文來回問題。\"},    # System prompt\n    {\"role\": \"user\", \"content\": test_question}, # User prompt\n]\n\nprint(generate_response(llama3, messages))","metadata":{"_uuid":"12ca583a-4379-4cf0-9db4-b63798d42c51","_cell_guid":"a676c9ad-c3af-48b0-8ccb-54cfb9ef785b","trusted":true,"collapsed":false,"id":"8dmGCARd_Oln","execution":{"iopub.status.busy":"2025-03-06T16:59:17.624270Z","iopub.status.idle":"2025-03-06T16:59:17.624565Z","shell.execute_reply":"2025-03-06T16:59:17.624428Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Agents","metadata":{"_uuid":"03653b8e-8ca6-4336-8fbb-24b3ed79e2b6","_cell_guid":"c54f378a-283a-48d5-982f-4d8b3961bff5","trusted":true,"collapsed":false,"id":"C0-ojJuE_Oln","jupyter":{"outputs_hidden":false}}},{"cell_type":"markdown","source":"The TA has implemented the Agent class for you. You can use this class to create agents that can interact with the LLM model. The Agent class has the following attributes and methods:\n- Attributes:\n    - role_description: The role of the agent. For example, if you want this agent to be a history expert, you can set the role_description to \"You are a history expert. You will only answer questions based on what really happened in the past. Do not generate any answer if you don't have reliable sources.\".\n    - task_description: The task of the agent. For example, if you want this agent to answer questions only in yes/no, you can set the task_description to \"Please answer the following question in yes/no. Explanations are not needed.\"\n    - llm: Just an indicator of the LLM model used by the agent.\n- Method:\n    - inference: This method takes a message as input and returns the generated response from the LLM model. The message will first be formatted into proper input for the LLM model. (This is where you can set some global instructions like \"Please speak in a polite manner\" or \"Please provide a detailed explanation\".) The generated response will be returned as the output.","metadata":{"_uuid":"48d3d82c-647c-40eb-8463-8c1fac478940","_cell_guid":"f2a503a5-0093-4df9-af0a-69df232a1753","trusted":true,"collapsed":false,"id":"HGsIPud3_Oln","jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"class LLMAgent():\n    def __init__(self, role_description: str, task_description: str, llm:str=\"bartowski/Meta-Llama-3.1-8B-Instruct-GGUF\"):\n        self.role_description = role_description   # Role means who this agent should act like. e.g. the history expert, the manager......\n        self.task_description = task_description    # Task description instructs what task should this agent solve.\n        self.llm = llm  # LLM indicates which LLM backend this agent is using.\n    def inference(self, message:str) -> str:\n        if self.llm == 'bartowski/Meta-Llama-3.1-8B-Instruct-GGUF': # If using the default one.\n            # TODO: Design the system prompt and user prompt here.\n            # Format the messsages first.\n            messages = [\n                {\"role\": \"system\", \"content\": f\"{self.role_description}\"},  # Hint: you may want the agents to speak Traditional Chinese only.\n                {\"role\": \"user\", \"content\": f\"{self.task_description}\\n{message}\"}, # Hint: you may want the agents to clearly distinguish the task descriptions and the user messages. A proper seperation text rather than a simple line break is recommended.\n            ]\n            return generate_response(llama3, messages)\n        else:\n            # TODO: If you want to use LLMs other than the given one, please implement the inference part on your own.\n            return \"\"","metadata":{"_uuid":"983e4cd1-9e69-486d-a1c3-9e6ed65cb762","_cell_guid":"7e6e4ca4-b701-4eb8-9514-0c28854c5287","trusted":true,"collapsed":false,"id":"zjG-UwDX_Oln","execution":{"iopub.status.busy":"2025-03-06T16:59:17.625527Z","iopub.status.idle":"2025-03-06T16:59:17.625826Z","shell.execute_reply":"2025-03-06T16:59:17.625713Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"TODO 1: Design the role description and task description for each agent.","metadata":{"_uuid":"4c57d25a-69f7-4848-8120-760573d00dcf","_cell_guid":"dadda9ff-d487-4f6b-822f-8844fe89a2fb","trusted":true,"collapsed":false,"id":"0-ueJrgP_Oln","jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"# TODO: Design the role and task description for each agent.\n\n# This agent may help you filter out the irrelevant parts in question descriptions.\nquestion_extraction_agent = LLMAgent(\n    role_description=\"你是一位非常擅長分析使用者的輸入、統整重要的資訊，並萃取出核心問題的AI助理。\",\n    task_description=\"請你從一段可能帶有大量背景資訊、敘述或多重子問題的使用者輸入中，「擷取關鍵的資訊、最需被回答的核心問題」，並將其簡化為一條精鍊且明確的問句。\"\n    \"如果問句本身就很簡短，而且問句的內容真的都是對最後答案會有重要影響的文字，可以把原始的問題直接當作核心問題\",\n)\n\n# This agent may help you extract the keywords in a question so that the search tool can find more accurate results.\nkeyword_extraction_agent = LLMAgent(\n    role_description=\"你是一位專門負責從一段文字或一句問題中，抽取出最能幫助搜尋或檢索的「關鍵字」或「重點詞彙」，確保後續檢索過程能更精準地取得相關資訊的AI助理。\",\n    task_description=\"請你從核心問題中，抽取並輸出能有效用於搜尋引擎的關鍵字或短語，把不同的關鍵字用空格隔開。\",\n)\n\n# This agent is the core component that answers the question.\nqa_agent = LLMAgent(\n    role_description=\"你是可以根據收集到的資訊來正確回答問題的AI。使用中文時一定只會使用繁體中文來回答問題，禁止使用簡體中文。\",\n    task_description=\"你會得到一個「搜尋到的內容 + 最終問題」的組合，請先認真看懂你需要回答的問題，再詳細閱讀提供給你的資訊，並回答出問題的答案。你的回答對我來說非常重要，請你盡全力回答正確，並且一定絕對要用繁體中文回答，禁止使用簡體中文。\",\n)\n","metadata":{"_uuid":"85780cc6-bef1-4841-87d7-43c868af3555","_cell_guid":"494450e8-b73f-4bb7-96df-9fb94d1fda3c","trusted":true,"collapsed":false,"id":"DzPzmNnj_Oln","execution":{"iopub.status.busy":"2025-03-06T16:59:17.626655Z","iopub.status.idle":"2025-03-06T16:59:17.626974Z","shell.execute_reply":"2025-03-06T16:59:17.626866Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## RAG pipeline","metadata":{"_uuid":"9d044642-804e-4518-ae81-dd5ef17b8935","_cell_guid":"141ad4bf-5143-4d38-85fd-af3d46273216","trusted":true,"collapsed":false,"id":"A9eoywr7_Oln","jupyter":{"outputs_hidden":false}}},{"cell_type":"markdown","source":"TODO 2: Implement the RAG pipeline.","metadata":{"_uuid":"18c1732b-5554-4c04-9211-4e310114a9f4","_cell_guid":"1985d6b8-177d-4e0c-952b-ed00c00de014","trusted":true,"collapsed":false,"id":"8HDOjNYJ_Oln","jupyter":{"outputs_hidden":false}}},{"cell_type":"markdown","source":"Please refer to the homework description slides for hints.\n\nAlso, there might be more heuristics (e.g. classifying the questions based on their lengths, determining if the question need a search or not, reconfirm the answer before returning it to the user......) that are not shown in the flow charts. You can use your creativity to come up with a better solution!","metadata":{"_uuid":"a2f9369c-a4c7-4f23-a983-2427bf0e2778","_cell_guid":"33a68fb2-a4c6-4277-b87f-e51c79a9a184","trusted":true,"collapsed":false,"id":"MRGNa-1i_Oln","jupyter":{"outputs_hidden":false}}},{"cell_type":"markdown","source":"- Naive approach (simple baseline)\n\n    ![](https://www.csie.ntu.edu.tw/~ulin/naive.png)","metadata":{"_uuid":"6a0b6b13-4342-4f86-ae77-04c951970f0c","_cell_guid":"f9cc8beb-30bf-40ce-94d3-aad026f526bb","trusted":true,"collapsed":false,"id":"cMaIsKAZ_Olo","jupyter":{"outputs_hidden":false}}},{"cell_type":"markdown","source":"- Naive RAG approach (medium baseline)\n\n    ![](https://www.csie.ntu.edu.tw/~ulin/naive_rag.png)","metadata":{"_uuid":"919bf318-91a8-4521-99e2-f9f7e867ce9f","_cell_guid":"2074d879-4fdf-4493-9f6a-65351abfe9f6","trusted":true,"collapsed":false,"id":"mppO-oOO_Olo","jupyter":{"outputs_hidden":false}}},{"cell_type":"markdown","source":"- RAG with agents (strong baseline)\n\n    ![](https://www.csie.ntu.edu.tw/~ulin/rag_agent.png)","metadata":{"_uuid":"42dd0dac-ec35-4fee-84a0-83b5b9099295","_cell_guid":"40d0afde-9d47-447a-a355-3423613e1a59","trusted":true,"collapsed":false,"id":"HYxbciLO_Olo","jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"async def pipeline(question: str) -> str:\n    \"\"\"\n    Strong Baseline:\n    1) question_extraction_agent：萃取核心問題\n    2) keyword_extraction_agent：取得關鍵字\n    3) 利用搜尋工具 search(keywords)，拿到外部文本\n    4) 將 外部文本 + 核心問題 ，餵給 qa_agent 得到最終答案\n    \"\"\"\n    print(\"原始問題：\", question)\n    # (A) 先從原始問題中，萃取出「最核心的問題」\n    core_question = question_extraction_agent.inference(question).strip()\n    print(\"核心問題：\", core_question)\n    # (B) 從核心問題中，進一步抽取出適合搜尋的關鍵字\n    keywords = keyword_extraction_agent.inference(question).strip()\n    # 例如可能是 \"熊仔, 碩班, 指導教授\" 或 \"熊仔 碩士 指導教授\"\n    print(\"關鍵字：\", keywords)\n    # (C) 使用搜尋工具，取得外部文本 (最多3筆)。也可視情況縮小/放大搜尋數量\n    #    若對流量或被 Google 暫時鎖定有疑慮，可改小一點，如 n_results=1 或 2。\n    search_results_list = await search(keywords, n_results=2)\n    \n    # (D) 將多筆文本合併成一個長字串，注意可做長度截斷 (避免超過 16384 tokens)\n    combined_context = \"\\n\\n\".join(search_results_list)\n    # 在此做截斷，避免 prompt 過長 (可以自行決定合理上限)\n    max_context_length = 12000 #可微調\n    combined_context = combined_context[:max_context_length]\n\n    # (E) 將「搜尋到的內容 + 最終問題」組合交給 QA agent 產生答案\n    final_prompt = (\n        \"以下是搜尋工具找到的相關內容：\\n\"\n        f\"{combined_context}\\n\\n\"\n        \"請依照上面資訊，回答此問題：\"\n        f\"{core_question}\\n\"\n    )\n\n    final_answer = qa_agent.inference(final_prompt)\n    return final_answer\n","metadata":{"_uuid":"4b235a2b-4029-444f-aa10-9b23e5b294c9","_cell_guid":"73428be8-bb29-459c-8315-40bcd82784f3","trusted":true,"collapsed":false,"id":"ztJkA7R7_Olo","execution":{"iopub.status.busy":"2025-03-06T16:59:17.627894Z","iopub.status.idle":"2025-03-06T16:59:17.628213Z","shell.execute_reply":"2025-03-06T16:59:17.628077Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Answer the questions using your pipeline!","metadata":{"_uuid":"42416041-dd4e-46e6-806e-bafbba503c6c","_cell_guid":"23c7c492-40c0-42f8-afe9-705f9f0de159","trusted":true,"collapsed":false,"id":"P_kI_9EGB0S9","jupyter":{"outputs_hidden":false}}},{"cell_type":"markdown","source":"Since Colab has usage limit, you might encounter the disconnections. The following code will save your answer for each question. If you have mounted your Google Drive as instructed, you can just rerun the whole notebook to continue your process.","metadata":{"_uuid":"06edba76-0a01-4100-8f3c-4a2b56e6c485","_cell_guid":"ff7b0d92-aa9c-4f02-b189-17b6f24ae113","trusted":true,"collapsed":false,"id":"PN17sSZ8DUg7","jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"from pathlib import Path\n\n# Fill in your student ID first.\nSTUDENT_ID = \"b12902026\"\n\nSTUDENT_ID = STUDENT_ID.lower()\nwith open('./public.txt', 'r') as input_f:\n    questions = input_f.readlines()\n    questions = [l.strip().split(',')[0] for l in questions]\n    for id, question in enumerate(questions, 1):\n        if Path(f\"./{STUDENT_ID}_{id}.txt\").exists():\n            continue\n        answer = await pipeline(question)\n        answer = answer.replace('\\n',' ')\n        print(id, answer)\n        with open(f'./{STUDENT_ID}_{id}.txt', 'w') as output_f:\n            print(answer, file=output_f)\n\nwith open('./private.txt', 'r') as input_f:\n    questions = input_f.readlines()\n    for id, question in enumerate(questions, 31):\n        if Path(f\"./{STUDENT_ID}_{id}.txt\").exists():\n            continue\n        answer = await pipeline(question)\n        answer = answer.replace('\\n',' ')\n        print(id, answer)\n        with open(f'./{STUDENT_ID}_{id}.txt', 'a') as output_f:\n            print(answer, file=output_f)","metadata":{"_uuid":"348e64c8-5b53-40e3-a4ba-431e28af7df4","_cell_guid":"0ca514a7-df72-46e2-9990-9537500080ed","trusted":true,"collapsed":false,"id":"plUDRTi_B39S","execution":{"iopub.status.busy":"2025-03-06T16:59:17.629184Z","iopub.status.idle":"2025-03-06T16:59:17.629553Z","shell.execute_reply":"2025-03-06T16:59:17.629381Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Combine the results into one file.\nwith open(f'./{STUDENT_ID}.txt', 'w') as output_f:\n    for id in range(1,91):\n        with open(f'./{STUDENT_ID}_{id}.txt', 'r') as input_f:\n            answer = input_f.readline().strip()\n            print(answer, file=output_f)","metadata":{"_uuid":"529b32a3-69cc-4f5f-9d8a-754fd5ee22f6","_cell_guid":"8a86dee8-1d22-4ae7-9d23-0aa1ee16264d","trusted":true,"collapsed":false,"id":"GmLO9PlmEBPn","execution":{"iopub.status.busy":"2025-03-06T16:59:17.630600Z","iopub.status.idle":"2025-03-06T16:59:17.630907Z","shell.execute_reply":"2025-03-06T16:59:17.630802Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null}]}